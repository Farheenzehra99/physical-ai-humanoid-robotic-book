# Data Model: Complete Retrieval Pipeline with Testing

**Feature**: 003-complete-retrieval-pipeline
**Date**: 2025-12-13
**Status**: Design

## Overview

This document defines the data models and entities for the complete retrieval pipeline. All models use Python dataclasses for type safety and validation.

---

## Core Entities

### 1. Query

Represents a user's natural language question submitted for retrieval.

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class Query:
    """User's natural language question for retrieval."""

    text: str
    top_k: int = 10
    score_threshold: Optional[float] = None

    def validate(self) -> None:
        """Validate query parameters."""
        if not self.text or not self.text.strip():
            raise ValueError("Query text cannot be empty")

        if self.top_k < 1 or self.top_k > 20:
            raise ValueError("top_k must be between 1 and 20")

        if self.score_threshold is not None:
            if not 0.0 <= self.score_threshold <= 1.0:
                raise ValueError("score_threshold must be between 0.0 and 1.0")
```

**Fields:**
- `text` (str, required): The natural language question
- `top_k` (int, default=10): Number of results to return
- `score_threshold` (Optional[float], default=None): Minimum similarity score filter

**Validation Rules:**
- Text must not be empty or whitespace-only
- top_k must be between 1 and 20 (inclusive)
- score_threshold must be between 0.0 and 1.0 if provided

**State Transitions:**
- Created → Validated → Embedded → Searched

---

### 2. SemanticEmbedding

Vector representation of a query generated by the embedding model.

```python
from dataclasses import dataclass
from typing import List
from datetime import datetime

@dataclass
class SemanticEmbedding:
    """Vector representation of query for similarity search."""

    vector: List[float]
    model: str
    generated_at: str
    dimension: int

    @classmethod
    def from_cohere_response(cls, response, model: str = "embed-english-v3.0"):
        """Create from Cohere API response."""
        vector = response.embeddings[0]
        return cls(
            vector=vector,
            model=model,
            generated_at=datetime.utcnow().isoformat(),
            dimension=len(vector)
        )

    def validate(self) -> None:
        """Validate embedding properties."""
        if self.dimension != 1024:
            raise ValueError(f"Expected dimension 1024, got {self.dimension}")

        if not all(isinstance(v, (int, float)) for v in self.vector):
            raise ValueError("All vector elements must be numeric")
```

**Fields:**
- `vector` (List[float], required): The 1024-dimensional embedding vector
- `model` (str, required): Embedding model used ("embed-english-v3.0")
- `generated_at` (str, required): ISO 8601 timestamp of generation
- `dimension` (int, required): Vector dimensionality (must be 1024)

**Validation Rules:**
- Dimension must be exactly 1024 (Cohere embed-english-v3.0 standard)
- All vector elements must be numeric (int or float)
- Vector should be normalized (cosine similarity requirement)

---

### 3. DocumentChunk

A segment of text from the knowledge base with associated metadata.

```python
from dataclasses import dataclass
from typing import Dict, Any

@dataclass
class DocumentChunk:
    """A document chunk stored in the vector database."""

    id: str
    content: str
    source_url: str
    title: str
    heading: str
    chunk_index: int
    metadata: Dict[str, Any]

    @classmethod
    def from_qdrant_payload(cls, point_id: str, payload: Dict[str, Any]):
        """Create from Qdrant search result payload."""
        return cls(
            id=payload.get('id', str(point_id)),
            content=payload.get('content', ''),
            source_url=payload.get('source_url', ''),
            title=payload.get('title', ''),
            heading=payload.get('heading', ''),
            chunk_index=payload.get('chunk_index', 0),
            metadata=payload.get('metadata', {})
        )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            'id': self.id,
            'content': self.content,
            'source_url': self.source_url,
            'title': self.title,
            'heading': self.heading,
            'chunk_index': self.chunk_index,
            'metadata': self.metadata
        }
```

**Fields:**
- `id` (str, required): Unique identifier for the chunk
- `content` (str, required): Full text content of the chunk
- `source_url` (str, required): Original URL of the source document
- `title` (str, required): Document title
- `heading` (str, required): Section heading within the document
- `chunk_index` (int, required): Position of chunk in original document (0-indexed)
- `metadata` (Dict[str, Any], required): Additional metadata (embedding_model, created_at, etc.)

**Invariants:**
- ID must be unique across all chunks in collection
- Content should not be empty
- chunk_index must be non-negative

---

### 4. RetrievalResult

A document chunk with its similarity score, representing search result relevance.

```python
from dataclasses import dataclass

@dataclass
class RetrievalResult:
    """A single retrieval result with similarity score."""

    chunk: DocumentChunk
    score: float
    rank: int

    @classmethod
    def from_qdrant_result(cls, qdrant_result, rank: int):
        """Create from Qdrant search result."""
        chunk = DocumentChunk.from_qdrant_payload(
            point_id=qdrant_result.id,
            payload=qdrant_result.payload
        )
        return cls(
            chunk=chunk,
            score=float(qdrant_result.score),
            rank=rank
        )

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary representation."""
        return {
            'rank': self.rank,
            'score': self.score,
            'chunk_id': self.chunk.id,
            'content': self.chunk.content,
            'source_url': self.chunk.source_url,
            'title': self.chunk.title,
            'heading': self.chunk.heading,
            'chunk_index': self.chunk.chunk_index,
            'metadata': self.chunk.metadata
        }

    def is_relevant(self, threshold: float = 0.78) -> bool:
        """Check if result meets relevance threshold."""
        return self.score >= threshold
```

**Fields:**
- `chunk` (DocumentChunk, required): The retrieved document chunk
- `score` (float, required): Similarity score (0.0 to 1.0, higher is better)
- `rank` (int, required): Position in ranked results (1-indexed)

**Validation Rules:**
- Score must be between 0.0 and 1.0
- Rank must be positive (1, 2, 3, ...)
- Results should be sorted by score (descending) when in a list

**Methods:**
- `is_relevant(threshold)`: Check if score meets minimum threshold

---

### 5. TestCase

A predefined query with expected quality criteria for validation.

```python
from dataclasses import dataclass, field
from typing import List, Optional
from enum import Enum

class QueryType(Enum):
    """Category of query for test organization."""
    FACTUAL = "factual"
    CONCEPTUAL = "conceptual"
    CODE_RELATED = "code_related"
    PROCEDURAL = "procedural"
    MULTI_TOPIC = "multi_topic"
    EDGE_CASE = "edge_case"

@dataclass
class TestCase:
    """A test query with expected results and validation criteria."""

    id: str
    query: str
    query_type: QueryType
    expected_keywords: List[str]
    min_score: float = 0.78
    description: Optional[str] = None
    expected_chunk_ids: List[str] = field(default_factory=list)

    def validate_results(self, results: List[RetrievalResult]) -> Dict[str, Any]:
        """Validate retrieval results against expectations."""
        if not results:
            return {
                'passed': False,
                'reason': 'No results returned',
                'score': 0.0
            }

        # Check minimum score
        top_score = results[0].score
        score_passed = top_score >= self.min_score

        # Check keyword presence
        keywords_found = []
        for keyword in self.expected_keywords:
            for result in results[:3]:  # Check top 3
                if keyword.lower() in result.chunk.content.lower():
                    keywords_found.append(keyword)
                    break

        keyword_coverage = len(keywords_found) / len(self.expected_keywords)
        keywords_passed = keyword_coverage >= 0.5  # At least 50% of keywords

        passed = score_passed and keywords_passed

        return {
            'passed': passed,
            'top_score': top_score,
            'min_score_met': score_passed,
            'keywords_found': keywords_found,
            'keyword_coverage': keyword_coverage,
            'keywords_met': keywords_passed,
            'num_results': len(results)
        }
```

**Fields:**
- `id` (str, required): Unique test case identifier
- `query` (str, required): The test query text
- `query_type` (QueryType, required): Category of query
- `expected_keywords` (List[str], required): Keywords expected in relevant results
- `min_score` (float, default=0.78): Minimum acceptable similarity score
- `description` (Optional[str]): Human-readable description of test intent
- `expected_chunk_ids` (List[str]): Specific chunks expected in results (optional)

**Validation Rules:**
- ID must be unique across all test cases
- Query must not be empty
- min_score must be between 0.0 and 1.0
- expected_keywords list should not be empty

---

## Supporting Models

### 6. RetrievalMetrics

Performance and quality metrics for retrieval operations.

```python
from dataclasses import dataclass
from typing import List, Optional

@dataclass
class RetrievalMetrics:
    """Performance and quality metrics for a retrieval operation."""

    # Timing metrics (milliseconds)
    total_latency_ms: float
    embedding_latency_ms: float
    search_latency_ms: float
    formatting_latency_ms: float

    # Quality metrics
    num_results: int
    avg_score: float
    max_score: float
    min_score: float

    # Outcome
    success: bool
    error_message: Optional[str] = None

    @classmethod
    def calculate(
        cls,
        results: List[RetrievalResult],
        timings: Dict[str, float],
        success: bool = True,
        error: Optional[str] = None
    ):
        """Calculate metrics from retrieval results and timings."""
        if not results:
            return cls(
                total_latency_ms=timings.get('total', 0),
                embedding_latency_ms=timings.get('embedding', 0),
                search_latency_ms=timings.get('search', 0),
                formatting_latency_ms=timings.get('formatting', 0),
                num_results=0,
                avg_score=0.0,
                max_score=0.0,
                min_score=0.0,
                success=success,
                error_message=error
            )

        scores = [r.score for r in results]

        return cls(
            total_latency_ms=timings.get('total', 0),
            embedding_latency_ms=timings.get('embedding', 0),
            search_latency_ms=timings.get('search', 0),
            formatting_latency_ms=timings.get('formatting', 0),
            num_results=len(results),
            avg_score=sum(scores) / len(scores),
            max_score=max(scores),
            min_score=min(scores),
            success=success,
            error_message=error
        )

    def meets_latency_target(self, target_ms: float = 800.0) -> bool:
        """Check if latency meets target (SC-003)."""
        return self.total_latency_ms < target_ms

    def meets_score_target(self, target: float = 0.78) -> bool:
        """Check if max score meets target (SC-002)."""
        return self.max_score >= target
```

**Fields:**
- Timing metrics: total, embedding, search, formatting latency (all in ms)
- Quality metrics: num_results, avg/max/min scores
- Outcome: success flag, optional error message

**Methods:**
- `calculate()`: Compute metrics from results and timings
- `meets_latency_target()`: Validate against SC-003 (< 800ms)
- `meets_score_target()`: Validate against SC-002 (≥ 0.78)

---

### 7. TestSuiteResults

Aggregate results from running the complete test suite.

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any
from datetime import datetime

@dataclass
class TestSuiteResults:
    """Aggregate results from test suite execution."""

    run_id: str
    timestamp: str
    total_tests: int
    passed_tests: int
    failed_tests: int
    test_results: List[Dict[str, Any]] = field(default_factory=list)
    aggregate_metrics: Dict[str, float] = field(default_factory=dict)

    @classmethod
    def create(cls, run_id: str = None):
        """Create new test suite results."""
        return cls(
            run_id=run_id or datetime.utcnow().strftime("%Y%m%d_%H%M%S"),
            timestamp=datetime.utcnow().isoformat(),
            total_tests=0,
            passed_tests=0,
            failed_tests=0
        )

    def add_test_result(self, test_case: TestCase, validation: Dict[str, Any]):
        """Add individual test result."""
        self.total_tests += 1
        if validation['passed']:
            self.passed_tests += 1
        else:
            self.failed_tests += 1

        self.test_results.append({
            'test_id': test_case.id,
            'query': test_case.query,
            'query_type': test_case.query_type.value,
            'validation': validation
        })

    def calculate_aggregates(self):
        """Calculate aggregate metrics across all tests."""
        if not self.test_results:
            return

        scores = [r['validation']['top_score'] for r in self.test_results
                 if 'top_score' in r['validation']]

        self.aggregate_metrics = {
            'success_rate': self.passed_tests / self.total_tests,
            'avg_top_score': sum(scores) / len(scores) if scores else 0.0,
            'median_score': sorted(scores)[len(scores)//2] if scores else 0.0,
            'min_score': min(scores) if scores else 0.0,
            'max_score': max(scores) if scores else 0.0
        }

    def meets_success_criteria(self) -> bool:
        """Check if results meet spec requirements (SC-002: 8/10)."""
        return self.passed_tests / self.total_tests >= 0.8

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary for JSON serialization."""
        return {
            'run_id': self.run_id,
            'timestamp': self.timestamp,
            'summary': {
                'total_tests': self.total_tests,
                'passed': self.passed_tests,
                'failed': self.failed_tests,
                'success_rate': self.passed_tests / self.total_tests if self.total_tests > 0 else 0
            },
            'aggregate_metrics': self.aggregate_metrics,
            'test_results': self.test_results
        }
```

**Fields:**
- `run_id`: Unique identifier for test run
- `timestamp`: ISO 8601 timestamp of test execution
- `total_tests`, `passed_tests`, `failed_tests`: Test counters
- `test_results`: List of individual test outcomes
- `aggregate_metrics`: Computed aggregate statistics

**Methods:**
- `add_test_result()`: Record individual test outcome
- `calculate_aggregates()`: Compute summary statistics
- `meets_success_criteria()`: Check against SC-002 (80% pass rate)

---

## Relationships

### Entity Relationship Diagram

```
┌──────────┐
│  Query   │
└────┬─────┘
     │ 1:1
     ▼
┌─────────────────┐
│SemanticEmbedding│
└────┬────────────┘
     │ 1:N
     ▼
┌──────────────────┐      ┌───────────────┐
│RetrievalResult(s)│◄─────┤DocumentChunk  │
└──────────────────┘  N:1 └───────────────┘
     │
     │ N:1
     ▼
┌──────────────────┐
│RetrievalMetrics  │
└──────────────────┘

┌──────────┐      ┌──────────────────┐
│TestCase  │─────►│RetrievalResult(s)│
└──────────┘ 1:N  └──────────────────┘
     │
     │ N:1
     ▼
┌──────────────────┐
│TestSuiteResults  │
└──────────────────┘
```

### Workflow Sequence

1. **Query Creation**: User creates Query object with text and parameters
2. **Embedding Generation**: Query → SemanticEmbedding (via Cohere API)
3. **Vector Search**: SemanticEmbedding → List[DocumentChunk] (via Qdrant)
4. **Result Ranking**: DocumentChunk + scores → List[RetrievalResult]
5. **Metrics Collection**: RetrievalResult(s) → RetrievalMetrics
6. **Validation** (optional): TestCase + RetrievalResult(s) → validation outcome
7. **Aggregation** (testing): Multiple tests → TestSuiteResults

---

## Data Validation

### Input Validation

All data models implement validation via:
- Type hints (enforced by mypy)
- Dataclass field constraints
- Custom `validate()` methods
- Factory methods with validation (`from_*` classmethods)

### Error Handling

```python
class ValidationError(Exception):
    """Raised when data validation fails."""
    pass

class RetrievalError(Exception):
    """Base exception for retrieval operations."""
    pass

class EmbeddingError(RetrievalError):
    """Raised when embedding generation fails."""
    pass

class SearchError(RetrievalError):
    """Raised when vector search fails."""
    pass
```

---

## Persistence

### Vector Database Schema (Qdrant)

**Collection**: `docusaurus_chunks`

**Vector Configuration**:
- Size: 1024 (Cohere embed-english-v3.0)
- Distance: Cosine
- On-disk: Yes

**Payload Schema**:
```python
{
    "id": str,              # Chunk identifier
    "content": str,         # Full text content
    "source_url": str,      # Original document URL
    "title": str,           # Document title
    "heading": str,         # Section heading
    "chunk_index": int,     # Position in document
    "metadata": {           # Additional metadata
        "embedding_model": str,
        "embedding_created_at": str
    }
}
```

**Indexed Fields**:
- `source_url` (KEYWORD): For filtering by source
- `title` (TEXT): For text search
- `heading` (TEXT): For section search

### Test Data Storage

**File**: `test_queries.json`

**Format**:
```json
{
    "version": "1.0",
    "created": "2025-12-13",
    "test_cases": [
        {
            "id": "test_001",
            "query": "What is Physical AI?",
            "query_type": "factual",
            "expected_keywords": ["Physical AI", "embodied", "robotics"],
            "min_score": 0.78,
            "description": "Basic definitional query"
        }
    ]
}
```

---

## Summary

This data model provides:
- **Type Safety**: All entities use dataclasses with type hints
- **Validation**: Built-in validation methods for data integrity
- **Flexibility**: Optional fields and configurable thresholds
- **Testability**: Dedicated models for testing and validation
- **Metrics**: Comprehensive performance and quality tracking
- **Serialization**: JSON-compatible with `to_dict()` methods

All models align with spec requirements (FR-001 through FR-012, SC-001 through SC-007).
